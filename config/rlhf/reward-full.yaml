# model_args
model_name_or_path: /private/model/qwen/Qwen2.5-0.5B-Instruct
tokenizer_name_or_path: /private/model/qwen/Qwen2.5-0.5B-Instruct
use_fast: true

# data_args
train_file: data/reward_data.json
output_dir: output/Qwen2.5-0.5B-Reward
preprocessing_num_workers: 32
data_cache_dir: cache/reward
max_seq_length: 4096
template_name: qwen

# train_args
deepspeed: config/deepspeed/ds_config_zero2.json
num_train_epochs: 1
per_device_train_batch_size: 4
gradient_accumulation_steps: 2
seed: 3407
do_train: true
optim: adamw_apex_fused
learning_rate: 1.0e-5
lr_scheduler_type: cosine
warmup_ratio: 0.01
adam_beta1: 0.9
adam_beta2: 0.95
weight_decay: 0.1
max_grad_norm: 1.0
bf16: true
fp16: false
overwrite_output_dir: true
gradient_checkpointing: true
save_steps: 109
logging_steps: 10
logging_strategy: steps
save_strategy: steps
logging_first_step: true
report_to: tensorboard

# extra_train_args
task_type: reward
train_mode: full
use_flash_att: true

# eval
# per_device_eval_batch_size: 1
# eval_strategy: steps
# eval_steps: 10
