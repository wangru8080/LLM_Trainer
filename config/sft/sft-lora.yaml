# model_args
model_name_or_path: /private/model/qwen/Qwen2.5-14B-Instruct
tokenizer_name_or_path: /private/model/qwen/Qwen2.5-14B-Instruct

# data_args
train_file: data/ms_sft_data-v2.json
output_dir: output/Qwen2.5-14B-Instruct-MS-LoRA
preprocessing_num_workers: 32
data_cache_dir: cache/sft
max_seq_length: 4096
template_name: qwen
discard_long_sample: true

# train_args
deepspeed: config/deepspeed/ds_config_zero2.json
num_train_epochs: 2
per_device_train_batch_size: 2
gradient_accumulation_steps: 4
seed: 3407
do_train: true
optim: adamw_apex_fused
learning_rate: 1.0e-4
lr_scheduler_type: cosine
warmup_ratio: 0.01
adam_beta1: 0.9
adam_beta2: 0.95
weight_decay: 0.1
max_grad_norm: 1.0
bf16: true
fp16: false
overwrite_output_dir: true
gradient_checkpointing: true
save_steps: 492
logging_steps: 10
logging_strategy: steps
save_strategy: steps
logging_first_step: true
report_to: tensorboard

# extra_train_args
task_type: sft
train_mode: lora
use_flash_att: true
use_flash_attn_ce_loss: false
train_shuffle: false

lora_rank: 8
lora_alpha: 16
lora_dropout: 0.01
modules_to_save: embed_tokens,lm_head
trainable_params: embed,norm

# eval
# per_device_eval_batch_size: 1
# eval_strategy: steps
# eval_steps: 10
